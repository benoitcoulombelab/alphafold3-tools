import argparse
import glob
import json
import os
import re
import statistics
import sys
import concurrent.futures
from typing import Tuple

import tqdm

from pairs import Af3LocalInteractionScore


def readable_file(filepath: str):
  """Checks if a file exists and is readable, or if it's "-" for stdin."""
  if filepath == "-":
    return filepath # Special case for stdin/stdout path name

  if not os.path.exists(filepath):
    # Raise ArgumentTypeError to make argparse show a clean error message
    raise argparse.ArgumentTypeError(f"File not found: {filepath}")

  if not os.access(filepath, os.R_OK):
    raise argparse.ArgumentTypeError(f"File not readable: {filepath}")

  # If all checks pass, return the original string (the filepath)
  return filepath


def writable_path(filepath: str):
  """Checks if the path is potentially writable, or if it's '-' for stdout."""
  if filepath == "-":
    return filepath # Special case for stdin/stdout path name

  # Check if directory exists (and create it if you want, but simple check is enough)
  # A simple check for writability can be complex, so for most CLI tools,
  # just returning the string is fine, and let 'open()' handle the creation/error.
  # We'll just return the string for simplicity here.
  return filepath


def dir_path(string: str):
  if not string or os.path.isdir(string):
    return string
  else:
    raise NotADirectoryError(string)


METRICS = ["iptm", "ptm", "ranking_score", "lis", "best_lis"]


def main(argv: list[str] = None):
  parser = argparse.ArgumentParser(
      description="Extract ipTM score (or other) from summary confidence JSON files generated by AlphaFold 3.")
  parser.add_argument("-i", "--input", type=dir_path, default="",
                      help="Base directory to look for summary confidence JSON files  (default: current directory)")
  parser.add_argument("-o", "--output", type=writable_path, default="-",
                      help="Tab delimited output file containing scores  (default: standard output '-')")
  parser.add_argument("-m", "--metrics", nargs="+", choices=METRICS,
                      default=[METRICS[0]],
                      help="Metrics to output  (default: %(default)s)")
  parser.add_argument("-n", "--name",
                      default=r"([\w-]+)__([\w-]+)_summary_confidences",
                      help="Regular expression to obtain bait/target names based on confidence filename "
                           " (default: %(default)s)")
  parser.add_argument("-1", "--sequence1", type=int, default=1,
                      help="Index of sequence one in the *_data.json file (default: %(default)s)")
  parser.add_argument("-2", "--sequence2", type=int, default=2,
                      help="Index of sequence two in the *_data.json file (default: %(default)s)")
  parser.add_argument("-p", "--progress", action="store_true", default=False,
                      help="Show progress bar")
  parser.add_argument("-t", "--threads", type=int, default=1,
                      help="Number of threads to compute score in parallel (default: %(default)s)")
  parser.add_argument("-M", "--mapping", type=readable_file,
                      help="Tab delimited text file used to convert names")
  parser.add_argument("-S", "--source_column", type=int, default="1",
                      help="Column index of source names in mapping file - 1 means first column of file" +
                           "   (default: %(default)s)")
  parser.add_argument("-C", "--converted_column", type=int, default="2",
                      help="Column index of converted names in mapping file - 1 means first column of file" +
                           "   (default: %(default)s)")

  args = parser.parse_args(argv)

  af3_score(input_dir=args.input, output_file=args.output,
            name=args.name,
            metrics=args.metrics,
            sequence_one=args.sequence1 - 1,
            sequence_two=args.sequence2 - 1,
            progress=args.progress,
            mapping_file=args.mapping,
            source_column=args.source_column - 1,
            converted_column=args.converted_column - 1,
            threads=args.threads)


def af3_score(input_dir: str = "",
    output_file: str = "-", name: str = r"([\w-]+)__([\w-]+)",
    metrics: list[str] = None,
    sequence_one: int = 0, sequence_two: int = 1,
    progress: bool = False,
    mapping_file: str = None, source_column: int = 0,
    converted_column: int = 1,
    threads: int = 1):
  """
  Extract ipTM score (or other) from summary confidence JSON files generated by AlphaFold 3.

  :param input_dir: input directory
  :param output_file: output file
  :param name: regular expression to obtain protein/gene names based on confidence filename
  :param metrics: metrics to output
  :param sequence_one: index of sequence one in the *_data.json file
  :param sequence_two: index of sequence two in the *_data.json file
  :param progress: if True, show progress bar
  :param mapping_file: tab delimited text file used to convert names
  :param source_column: column index of source names in mapping file
  :param converted_column: column index of converted names in mapping file
  :param threads: number of threads to compute score in parallel (default: 1)
  """
  if metrics is None:
    metrics = [METRICS[0]]
  if len(metrics) == 0:
    raise AssertionError("metrics must have at least one value")
  if len([metric for metric in metrics if metric not in METRICS]) > 0:
    raise AssertionError(
        f"metrics values must all be present in {METRICS}")
  if threads < 1:
    raise AssertionError("threads value must be at least 1")
  confidence_files = sorted(
      glob.glob("**/*_summary_confidences.json", root_dir=input_dir,
                recursive=True))
  confidence_files = [os.path.join(input_dir, confidence_file) for
                      confidence_file in confidence_files]
  mappings = {}
  if mapping_file:
    mappings = parse_mapping(mapping_file, source_column, converted_column)
  futures = []
  with concurrent.futures.ProcessPoolExecutor(max_workers=threads) as executor:
    futures += [executor.submit(executor_get_confidence_scores, confidence_file, metrics, sequence_one, sequence_two) for confidence_file in confidence_files]
    # Let tasks complete.
    if progress:
      with tqdm.tqdm(total=len(confidence_files)) as pbar:
        for future in concurrent.futures.as_completed(futures):
          pbar.update(1)
    else:
      for future in concurrent.futures.as_completed(futures):
        continue
  all_scores = [future.result() for future in futures]
  with open(output_file, "w") if output_file != "-" else sys.stdout as output_file_out:
    output_file_out.write("Bait\tTarget")
    for metric in metrics:
      if "iptm" == metric:
        output_file_out.write("\tipTM")
      elif "ptm" == metric:
        output_file_out.write("\tpTM")
      elif "ranking_score" == metric:
        output_file_out.write("\tRanking score")
      elif "lis" == metric:
        output_file_out.write("\tiLIS\tLIS\tLIA")
      elif "best_lis" == metric:
        output_file_out.write("\tBest iLIS\tBest LIS\tBest LIA")
    output_file_out.write("\n")
    for confidence_file, scores in all_scores:
      re_match = re.search(name, confidence_file)
      if not re_match:
        raise AssertionError(
            f"Expression {name} cannot be found in filename {confidence_file}")
      bait, target = re_match.group(1, 2)
      bait = mappings[bait] if bait in mappings else bait
      target = mappings[target] if target in mappings else target
      output_file_out.write(f"{bait}\t{target}")
      for score in scores:
        output_file_out.write(f"\t{score}")
      output_file_out.write("\n")


def executor_get_confidence_scores(confidence_file: str, metrics: list[str] = None,
    sequence_one: int = 0, sequence_two: int = 1) -> tuple[str, list[float]]:
  """
  Calls get_sequence_index than get_confidence_scores and returns confidence scores

  :param confidence_file: confidence JSON file
  :param metrics: metrics to obtain confidence scores
  :param sequence_one: index of sequence one in the *_data.json file
  :param sequence_two: index of sequence two in the *_data.json file
  :return: confidence_file and confidence scores
  """
  sequence_one_index, sequence_two_index = get_sequence_index(confidence_file, sequence_one, sequence_two)
  scores = get_confidence_scores(confidence_file, metrics, sequence_one_index, sequence_two_index)
  return confidence_file, scores


def get_confidence_scores(confidence_file: str, metrics: list[str] = None,
    sequence_one: int = 0, sequence_two: int = 1) -> list[float]:
  """
  Returns confidence scores for given metrics

  :param confidence_file: confidence JSON file
  :param metrics: metrics to obtain confidence scores
  :param sequence_one: index of sequence one
  :param sequence_two: index of sequence two
  :return: confidence scores
  """
  if metrics is None:
    metrics = [METRICS[0]]
  if len([metric for metric in metrics if metric not in METRICS]) > 0:
    raise AssertionError(
        f"metrics values must all be present in {METRICS}")

  with open(confidence_file, "r") as input_in:
    confidences = json.load(input_in)

  scores = []
  for metric in metrics:
    if "iptm" == metric:
      scores.append(confidences["iptm"])
    elif "ptm" == metric:
      scores.append(confidences["ptm"])
    elif "ranking_score" == metric:
      scores.append(confidences["ranking_score"])
    elif "lis" == metric:
      model_confidence_files = (
        glob.glob("**/confidences.json",
                  root_dir=os.path.dirname(confidence_file), recursive=True))
      model_confidence_files = [
        os.path.join(os.path.dirname(confidence_file), model_confidence_file)
        for model_confidence_file in model_confidence_files]
      structure_files = [
        model_confidence_file.replace("confidences.json", "model.cif") for
        model_confidence_file in model_confidence_files]
      model_lis = [Af3LocalInteractionScore.local_interaction_score(
          model_confidence_files[i], structure_files[i],
          subunit_one=sequence_one, subunit_two=sequence_two) for i in
        range(0, len(model_confidence_files))]
      i_lis = statistics.mean([m_lis[0] for m_lis in model_lis])
      lis = statistics.mean([m_lis[1] for m_lis in model_lis])
      lia = statistics.mean([m_lis[2] for m_lis in model_lis])
      scores.extend([i_lis, lis, lia])
    elif "best_lis" == metric:
      lis_json = confidence_file.replace("_summary_confidences.json",
                                         "_confidences.json")
      structure = confidence_file.replace("_summary_confidences.json",
                                          "_model.cif")
      i_lis, lis, lia = Af3LocalInteractionScore.local_interaction_score(
          lis_json, structure, subunit_one=sequence_one, subunit_two=sequence_two)
      scores.extend([i_lis, lis, lia])
  return scores


def get_sequence_index(confidence_file: str,
    sequence_one: int = 0, sequence_two: int = 1) -> Tuple[int, int]:
  """
  Returns index of sequence one and two from the *_data.json file for the *_confidences.json and *_model.cif files.

  :param confidence_file: confidence JSON file
  :param sequence_one: index of sequence one in the *_data.json file
  :param sequence_two: index of sequence two in the *_data.json file
  :return:
  """
  data_json = confidence_file.replace("_summary_confidences.json",
                                     "_data.json")
  confidences_json = confidence_file.replace("_summary_confidences.json",
                                      "_confidences.json")
  with open(data_json, "r") as data_json_in:
    data = json.load(data_json_in)
  with open(confidences_json, "r") as confidences_json_in:
    confidences = json.load(confidences_json_in)
  sequence_ids = list(dict.fromkeys(confidences["atom_chain_ids"]))
  sequence_type = list(data["sequences"][sequence_one].keys())[0]
  sequence_id = data["sequences"][sequence_one][sequence_type]["id"]
  sequence_one_index = sequence_ids.index(sequence_id)
  sequence_type = list(data["sequences"][sequence_two].keys())[0]
  sequence_id = data["sequences"][sequence_two][sequence_type]["id"]
  sequence_two_index = sequence_ids.index(sequence_id)
  return sequence_one_index, sequence_two_index


def parse_mapping(mapping_file: str, source_column: int = 0,
    converted_column: int = 1) \
    -> dict[str, str]:
  """
  Parse mapping file.

  :param mapping_file: text delimited filename
  :param source_column: index of source id columns
  :param converted_column: index of converted id columns
  :return: dictionary of source id to converted id
  """
  mappings = {}
  with open(mapping_file, "r") if mapping_file != "-" else sys.stdin as mapping_file_in:
    for line in mapping_file_in:
      if line.startswith("#"):
        continue
      columns = line.rstrip("\r\n").split("\t")
      source = columns[source_column].lower()
      converted = columns[converted_column]
      if converted:
        mappings[source] = converted
  return mappings


if __name__ == "__main__":
  main()
